# Machine Learning

**K-means 군집**

방법

- n_cluster 개수만큼 클러스터 중심을 무작위로 생성.
- 데이터 포인트를 가장 가까운 클러스터 중심에 할당.
- 할당된 데이터 포인트를 평균으로 클러스터 중심을 다시 지정.
- 할당된 데이터 포인트에 변화 없을 때까지 반복.

<br>

**sklearn에서의 k-평균 군집**

1. KMeans 객체 생성 & n_cluster 지정.
2. `fit` 메서드 호출.

분류와 비슷하지만, 정답을 모르고 레이블의 의미가 없다.

<br>

**k-평균 알고리즘이 실패하는 경우**

- k-평균 군집을 정의하는 건 중심 한 가지뿐. 그래서 범위가 원형으로 한정적임.
- k-평균은 클러스터에서 모든 방향이 똑같이 중요하다 가정.
  - 복잡한 데이터셋 분리 성능이 떨어짐.

<br>

**벡터 양자화**

k-평균 군집과 PCA/NMF는 유사점이 있다.

- PCA는 분산이 가장 큰 방향을 찾고, NMF는 극단/중첩된 성분을 찾는게 목표.
- k-평균 군집은 데이터 포인트를 하나의 성분으로 표현하는게 목표. 데이터를 하나의 성분으로 분해하는 것이 벡터 양자화.



PCA/NMF는 차원 축소. 잘못 축소했다간 데이터의 특성이 뭉개질수도 있음. 

- 반면에 k-평균 군집은 입력 특성 개수 이상으로 클러스터 생성 가능.

<br>

**k-평균 알고리즘 장단점**

- 장점: 이해 쉬움. 구현 쉬움. 구현 빠름.
- 단점: 무작위. 초기값에 따라 결과 달라짐. 범위가 한정적. n_cluster 정의 필요.

<br>

**병합 군집**

방법

- 각 데이터 포인트를 클러스터로 지정.
- 가장 비슷한 두 클러스터를 결합해나간다. linkage 설정으로 가장 비슷한 클러스터를 정의. linkage의 종류는 세 가지.
  - ward: 클러스터 내 분산이 가장 작게 증가되는 두 클러스터 결합. 크기가 대부분 비슷하게 형성됨.
  - average: 클러스터 데이터 사이 평균 거리가 최소인 두 클러스터 결합.
  - complete: 클러스터 데이터 사이 최대 거리가 최소인 두 클러스터 결합.



병합 군집은 새 데이터 포인트에 대해 예측할 수 없다.

- 클러스터 개수 잘 모를 때 해봄직 함.

<br>

**계층적 군집**

병합 군집으로 계층적 군집이 형성됨.

- 하나의 데이터 포인트로 구성된 클러스터에서 마지막 클러스터까지 이동.



2차원 데이터셋은 시각화 가능하지만, 그 이상은 덴드로그램으로 시각화.

- 데이터 포인트를 leaf로 하여 구성된 tree 구조.
- tree의 branch의 길이는 두 클러스터간 거리를 나타냄.



병합 군집은 복잡한 데이터셋에 분류가 효과적이지 않음.

<br>

**DBSCAN**

특성 공간에서 데이터들이 붐비는 밀집 지역을 찾고, 비어있는 지역을 경계로 다른 클러스터들과 구분.

- 주요 장점: n_cluster 지정할 필요 없고, 복잡한 형상 찾을 수 있고, 노이즈도 구별 가능.

<br>

**DBSCAN 알고리즘**

1. 무작위로 데이터 포인트 선택.
2. `eps` 범위 내에 데이터가 `min_samples`개 이상 있다면 core point, 없다면 border point.
3. `eps` 범위 내 이웃 데이터 살피기. 똑같이 `eps` 범위 내에 데이터가 `min_samples`개 이상 있다면 core point, 없다면 border point.
4. `eps` 범위 내에 데이터가 한 개도 없다면 noise.



결국 DBSCAN의 중요한 매개변수는 **eps** 와 **min_samples**.

