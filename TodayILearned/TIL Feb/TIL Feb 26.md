# Data Scaling

데이터 스케일링은

- 다차원의 값들을 비교하기 쉽게 만들어준다.

- 공분산 행렬의 조건수(conditional number)를 감소시켜 최적화 과정에서의 안정성 및 수렴 속도를 향상시킨다.

<br>

**조건수**

작은 변화의 비율에 대해 함수가 얼마나 변화할 수 있는지에 대한 argument measure.

- 조건수가 크면 약간의 오차만으로도 전혀 다른 값을 가진다.
- 

조건수가 커지는 경우

1. 단위 차이로 인한 scale difference. 이 경우 스케일링으로 해결.
2. 다중 공선성. 이 경우 변수 선택이나 PCA 사용.

- skewed된 함수의 경우 로그 함수나 제곱근 함수를 사용.

<br>

**데이터 스케일링 종류**

- StandardScaler: 평균을 제거하고 분산을 1로. 평균과 표준편차의 특성상 이상치에 민감하다.
- MinMaxScaler: 특성의 범위를 0~1로. 이상치가 있다면 매우 작은 범위가 생성됨. 민감하다.
- AbsMaxScaler: 데이터의 절댓값을 0~1로. MinMaxScaler와 성질이 동일.
- RobustScaler: 아웃라이어의 영향을 최소화한 기법. 평균, 분산 대신 중간값, IQR를 사용. 이상치에 덜 민감하다.



결론적으로, 모든 데이터 스케일링 과정에서 이상치는 변환 효과를 저해시킨다.

- 이상치가 단순 이상이라면 제거하고 스케일링을 진행하고, 이상치가 데이터의 특성이라면 그에 맞는 데이터 스케일링을 진행한다.

<br><br>

# Machine Learning

**주성분분석**

주성분을 완벽히 이해하는 것은 어렵지만 대충 주성분이 잡아낸 특징은 짐작해볼 수 있다.

- 알고리즘이 데이터를 해석하는 방식은 사람의 방식과는 다르다.



주성분을 해석하는 방법:

1. 테스트 포인트를 주성분의 가중치 합으로 나타내는 데 필요한 수치를 찾아 PCA의 의미를 찾을 수 있다. 
2. 주성분 몇 개를 사용해 원본 데이터를 구성해본다.

<br>

**NMF 비음수 행렬 분해**

양수인 행렬을 분해해 차원 축소하는 알고리즘.

- PCA와 유사 (어떤 성분의 가중치 합으로 데이터를 나타낼 수 있다).
- 그 대신 음수가 아닌 성분과 계수값. 
- 섞여 있는 데이터 (음원, 목소리) 추출에 유리하다.
- PCA와 다르게 모든 성분을 동등하게 취급한다.

이미지 데이터셋에서는 PCA가 성능이 더 좋았지만, 레코드 추출에서는 NMF가 더 효과적.

<br>

**t-SNE 매니폴드 학습**

매니폴드 학습은 복잡한 매핑을 만들어 더 나은 시각화를 제공한다.

- 훈련 데이터를 새로운 표현으로 변환시키지만, 새로운 데이터에는 적용하지 못한다.
- EDA는 유용하지만, 지도 학습용으로는 사용하지 않는다.



t-SNE 아이디어는 데이터 포인트 사이 거리를 가장 잘 보존하는 2차원 표현을 찾는 것. 

- 가까운건 가까운거 끼리, 먼건 먼 것끼리.



매개변수가 있긴 하지만, 효과가 크지않거니와 기본값으로도 잘 작동한다.

